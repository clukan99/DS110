{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the file daily.csv The file has ride data for the Durham, NC bus system over time\n",
    "\n",
    "Change day of week to a proper US ordered factor and show that it works using the table() function\n",
    "\n",
    "Use base R to create two new data frames (weekday and weekend) from the daily ride data with the appropriate observations.\n",
    "\n",
    "Use base R to create a  vector called winterRiders which include only the numbers of riders (n_riders) on weekends in the winter months of Jan, Feb, and Mar and produce a histogram of them.\n",
    "\n",
    "Use base R to do an even/odd (row number) split of the daily ride data into two dataframes; even and odd rows.\n",
    "\n",
    "Use base R to do a random (set seed to 440) sample split of the data into train (60%), test (40%) dataframes respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change day of week to a proper US ordered factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of      ride_date day_of_week month  n_rides  n_riders  n_unique_stops  \\\n",
       "0   2015-01-01       Thurs   Jan       58        37              44   \n",
       "1   2015-01-02         Fri   Jan      134        83              93   \n",
       "2   2015-01-03         Sat   Jan      145        84             100   \n",
       "3   2015-01-04         Sun   Jan      101        57              63   \n",
       "4   2015-01-05         Mon   Jan      182       117             109   \n",
       "..         ...         ...   ...      ...       ...             ...   \n",
       "359 2015-12-27         Sun   Dec      173        97             108   \n",
       "360 2015-12-28         Mon   Dec      322       175             176   \n",
       "361 2015-12-29        Tues   Dec      338       190             184   \n",
       "362 2015-12-30         Wed   Dec      317       184             172   \n",
       "363 2015-12-31       Thurs   Dec      301       167             166   \n",
       "\n",
       "     n_unique_routes  \n",
       "0                 14  \n",
       "1                 24  \n",
       "2                 25  \n",
       "3                 14  \n",
       "4                 23  \n",
       "..               ...  \n",
       "359               15  \n",
       "360               24  \n",
       "361               26  \n",
       "362               28  \n",
       "363               25  \n",
       "\n",
       "[364 rows x 7 columns]>"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily = pd.read_csv(\"../DATA/daily.csv\")\n",
    "daily['ride_date'] = pd.to_datetime(daily['ride_date'])\n",
    "daily.sort_values(by = 'ride_date')\n",
    "daily.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use base R to create two new data frames (weekday and weekend) from the daily ride data with the appropriate observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday = daily[daily['day_of_week'] != ('Sat' and 'Sun')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "winter_riders = daily[daily['month'] == (\"Jan\" or \"Feb\" or \"Mar\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "even = daily[daily.index % 2 == 0]\n",
    "odd =  daily[daily.index % 2 != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ride_date          0\n",
       "day_of_week        0\n",
       "month              0\n",
       "n_rides            0\n",
       "n_riders           0\n",
       "n_unique_stops     0\n",
       "n_unique_routes    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting categories to numbers\n",
    "day_of_week_array = array(daily['day_of_week'])\n",
    "label_encoder = LabelEncoder()\n",
    "day_of_week_array =label_encoder.fit_transform(day_of_week_array)\n",
    "daily['day_of_week'] = day_of_week_array\n",
    "\n",
    "#Converting numbers to bindary\n",
    "daily['day_of_week'] = daily['day_of_week'].map({0: \"Fri\", 1: \"Mon\", 2: \"Sat\", 3: \"Sun\", 4: \"Thurs\", 5: \"Tues\", 6: \"Wed\"})\n",
    "daily = pd.get_dummies(daily, columns=['day_of_week'], prefix='', prefix_sep='')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_array = array(daily['month'])\n",
    "label_encoder = LabelEncoder()\n",
    "month_array =label_encoder.fit_transform(month_array)\n",
    "daily['month'] = month_array\n",
    "\n",
    "#Converting numbers to bindary\n",
    "daily['month'] = daily['month'].map({0: \"Apr\", 1: \"Aug\", 2: \"Dec\", 3: \"Feb\", 4: \"Jan\", 5: \"Jul\", 6: \"Jun\",7: \"Mar\",8:\"May\",9:\"Nov\",10:\"Oct\",11:\"Sep\"})\n",
    "daily = pd.get_dummies(daily, columns=['month'], prefix='', prefix_sep='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(daily.columns)\n",
    "\n",
    "len(train_dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily['riders'] = daily['n_riders']\n",
    "daily = daily.drop(columns=['n_riders'])\n",
    "daily = daily.drop(columns=['ride_date'])\n",
    "train_dataset = daily.sample(frac=0.8, random_state=0)\n",
    "test_dataset = daily.drop(train_dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Converting to an array here\n",
    "train_val = train_dataset.values\n",
    "test_val = test_dataset.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_rides</th>\n",
       "      <th>n_unique_stops</th>\n",
       "      <th>n_unique_routes</th>\n",
       "      <th>Fri</th>\n",
       "      <th>Mon</th>\n",
       "      <th>Sat</th>\n",
       "      <th>Sun</th>\n",
       "      <th>Thurs</th>\n",
       "      <th>Tues</th>\n",
       "      <th>Wed</th>\n",
       "      <th>...</th>\n",
       "      <th>Feb</th>\n",
       "      <th>Jan</th>\n",
       "      <th>Jul</th>\n",
       "      <th>Jun</th>\n",
       "      <th>Mar</th>\n",
       "      <th>May</th>\n",
       "      <th>Nov</th>\n",
       "      <th>Oct</th>\n",
       "      <th>Sep</th>\n",
       "      <th>riders</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>465</td>\n",
       "      <td>241</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>399</td>\n",
       "      <td>207</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>244</td>\n",
       "      <td>126</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>106</td>\n",
       "      <td>23</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>29</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     n_rides  n_unique_stops  n_unique_routes  Fri  Mon  Sat  Sun  Thurs  \\\n",
       "308      465             241               28    1    0    0    0      0   \n",
       "258      399             207               27    0    0    0    0      1   \n",
       "45       244             126               25    0    1    0    0      0   \n",
       "26       106              23               13    0    0    0    0      0   \n",
       "219       29              22               12    0    0    0    1      0   \n",
       "\n",
       "     Tues  Wed  ...  Feb  Jan  Jul  Jun  Mar  May  Nov  Oct  Sep  riders  \n",
       "308     0    0  ...    0    0    0    0    0    0    1    0    0     284  \n",
       "258     0    0  ...    0    0    0    0    0    0    0    0    1     246  \n",
       "45      0    0  ...    1    0    0    0    0    0    0    0    0     164  \n",
       "26      1    0  ...    0    1    0    0    0    0    0    0    0      87  \n",
       "219     0    0  ...    0    0    0    0    0    0    0    0    0      17  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Col 3 is predict for n_rriders\n",
    "\n",
    "x_train =  train_val[:,:22]\n",
    "x_train = np.asarray(x_train)\n",
    "y_train =  train_val[:,22]\n",
    "y_train  = np.asarray(y_train)\n",
    "X_test =  test_val[:,:22]\n",
    "X_test = np.asarray(X_test)\n",
    "Y_test = test_val[:,22]\n",
    "Y_test = np.asarray(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/48\n",
      "5/5 [==============================] - 1s 46ms/step - loss: 23271.3672 - mse: 23271.3672 - mae: 140.6629 - val_loss: 20511.1973 - val_mse: 20511.1973 - val_mae: 133.9563\n",
      "Epoch 2/48\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 14349.2881 - mse: 14349.2881 - mae: 109.2601 - val_loss: 11615.4072 - val_mse: 11615.4072 - val_mae: 99.6069\n",
      "Epoch 3/48\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 7397.7964 - mse: 7397.7964 - mae: 75.0784 - val_loss: 4606.1338 - val_mse: 4606.1338 - val_mae: 59.5933\n",
      "Epoch 4/48\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 2667.3347 - mse: 2667.3347 - mae: 39.6252 - val_loss: 898.0458 - val_mse: 898.0458 - val_mae: 21.6169\n",
      "Epoch 5/48\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1007.3915 - mse: 1007.3915 - mae: 24.5917 - val_loss: 593.5795 - val_mse: 593.5795 - val_mae: 20.9894\n",
      "Epoch 6/48\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1375.4972 - mse: 1375.4972 - mae: 29.8536 - val_loss: 763.4769 - val_mse: 763.4769 - val_mae: 24.3881\n",
      "Epoch 7/48\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1368.2214 - mse: 1368.2214 - mae: 29.9285 - val_loss: 401.0639 - val_mse: 401.0639 - val_mae: 16.5207\n",
      "Epoch 8/48\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 867.0605 - mse: 867.0605 - mae: 23.2054 - val_loss: 236.9136 - val_mse: 236.9136 - val_mae: 12.4343\n",
      "Epoch 9/48\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 746.3392 - mse: 746.3392 - mae: 20.5151 - val_loss: 351.3997 - val_mse: 351.3997 - val_mae: 15.1747\n",
      "Epoch 10/48\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 803.7408 - mse: 803.7408 - mae: 20.5323 - val_loss: 361.7158 - val_mse: 361.7158 - val_mae: 15.4650\n",
      "Epoch 11/48\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 756.6514 - mse: 756.6514 - mae: 20.3054 - val_loss: 232.4727 - val_mse: 232.4727 - val_mae: 12.3414\n",
      "Epoch 12/48\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 727.7537 - mse: 727.7537 - mae: 20.4442 - val_loss: 161.6572 - val_mse: 161.6572 - val_mae: 10.3631\n",
      "Epoch 13/48\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 647.0801 - mse: 647.0801 - mae: 19.3767 - val_loss: 146.7818 - val_mse: 146.7818 - val_mae: 9.9355\n",
      "Epoch 14/48\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 703.0904 - mse: 703.0904 - mae: 20.1806 - val_loss: 148.8316 - val_mse: 148.8316 - val_mae: 10.0641\n",
      "Epoch 15/48\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 720.1110 - mse: 720.1110 - mae: 20.0555 - val_loss: 150.9647 - val_mse: 150.9647 - val_mae: 10.1540\n",
      "Epoch 16/48\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 598.9498 - mse: 598.9498 - mae: 18.7884 - val_loss: 175.2629 - val_mse: 175.2629 - val_mae: 10.9473\n",
      "Epoch 17/48\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 714.3650 - mse: 714.3650 - mae: 19.6685 - val_loss: 182.3430 - val_mse: 182.3430 - val_mae: 11.1422\n",
      "Epoch 18/48\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 614.8701 - mse: 614.8701 - mae: 18.5493 - val_loss: 161.2034 - val_mse: 161.2034 - val_mae: 10.4386\n",
      "Epoch 19/48\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 656.1735 - mse: 656.1735 - mae: 19.1410 - val_loss: 147.6809 - val_mse: 147.6809 - val_mae: 9.9893\n",
      "Epoch 20/48\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 552.7484 - mse: 552.7484 - mae: 17.8320 - val_loss: 156.7942 - val_mse: 156.7942 - val_mae: 10.3997\n",
      "Epoch 21/48\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 612.3875 - mse: 612.3875 - mae: 18.4495 - val_loss: 164.5473 - val_mse: 164.5473 - val_mae: 10.6686\n",
      "Epoch 22/48\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 534.9850 - mse: 534.9850 - mae: 17.4747 - val_loss: 158.8576 - val_mse: 158.8576 - val_mae: 10.4773\n",
      "Epoch 23/48\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 566.3734 - mse: 566.3734 - mae: 18.0438 - val_loss: 160.5933 - val_mse: 160.5933 - val_mae: 10.5327\n",
      "Epoch 24/48\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 643.5542 - mse: 643.5542 - mae: 19.0124 - val_loss: 162.0039 - val_mse: 162.0039 - val_mae: 10.5901\n",
      "Epoch 25/48\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 590.1996 - mse: 590.1996 - mae: 18.5080 - val_loss: 144.9711 - val_mse: 144.9711 - val_mae: 9.9785\n",
      "Epoch 26/48\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 608.2823 - mse: 608.2823 - mae: 18.2894 - val_loss: 146.5698 - val_mse: 146.5698 - val_mae: 10.0354\n",
      "Epoch 27/48\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 612.3060 - mse: 612.3060 - mae: 18.3999 - val_loss: 161.4632 - val_mse: 161.4632 - val_mae: 10.5743\n",
      "Epoch 28/48\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 568.5477 - mse: 568.5477 - mae: 17.8400 - val_loss: 156.3537 - val_mse: 156.3537 - val_mae: 10.4412\n",
      "Epoch 29/48\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 580.4350 - mse: 580.4350 - mae: 18.0540 - val_loss: 135.8970 - val_mse: 135.8970 - val_mae: 9.6227\n",
      "Epoch 30/48\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 567.5756 - mse: 567.5756 - mae: 18.1546 - val_loss: 144.5722 - val_mse: 144.5722 - val_mae: 9.9867\n",
      "Epoch 31/48\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 582.5131 - mse: 582.5131 - mae: 17.8382 - val_loss: 155.1315 - val_mse: 155.1315 - val_mae: 10.4111\n",
      "Epoch 32/48\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 559.9283 - mse: 559.9283 - mae: 17.9975 - val_loss: 138.0284 - val_mse: 138.0284 - val_mae: 9.7583\n",
      "Epoch 33/48\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 510.6471 - mse: 510.6471 - mae: 17.3139 - val_loss: 136.6430 - val_mse: 136.6430 - val_mae: 9.7113\n",
      "Epoch 34/48\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 559.6313 - mse: 559.6313 - mae: 17.9783 - val_loss: 126.2292 - val_mse: 126.2292 - val_mae: 9.2637\n",
      "Epoch 35/48\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 581.8556 - mse: 581.8556 - mae: 18.1133 - val_loss: 134.5042 - val_mse: 134.5042 - val_mae: 9.6447\n",
      "Epoch 36/48\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 517.5477 - mse: 517.5477 - mae: 17.1747 - val_loss: 138.6205 - val_mse: 138.6205 - val_mae: 9.8107\n",
      "Epoch 37/48\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 588.1810 - mse: 588.1810 - mae: 18.1635 - val_loss: 129.8322 - val_mse: 129.8322 - val_mae: 9.4427\n",
      "Epoch 38/48\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 534.9913 - mse: 534.9913 - mae: 17.6283 - val_loss: 116.1894 - val_mse: 116.1894 - val_mae: 8.8206\n",
      "Epoch 39/48\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 521.0076 - mse: 521.0076 - mae: 17.6321 - val_loss: 121.1385 - val_mse: 121.1385 - val_mae: 9.1108\n",
      "Epoch 40/48\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 495.7151 - mse: 495.7151 - mae: 16.9782 - val_loss: 159.8791 - val_mse: 159.8791 - val_mae: 10.6507\n",
      "Epoch 41/48\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 559.8560 - mse: 559.8560 - mae: 17.2764 - val_loss: 134.9962 - val_mse: 134.9962 - val_mae: 9.7571\n",
      "Epoch 42/48\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 526.0675 - mse: 526.0675 - mae: 17.3491 - val_loss: 108.5465 - val_mse: 108.5465 - val_mae: 8.4117\n",
      "Epoch 43/48\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 534.2946 - mse: 534.2946 - mae: 17.3753 - val_loss: 110.1246 - val_mse: 110.1246 - val_mae: 8.5435\n",
      "Epoch 44/48\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 570.2139 - mse: 570.2139 - mae: 17.8362 - val_loss: 117.2126 - val_mse: 117.2126 - val_mae: 8.9826\n",
      "Epoch 45/48\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 497.0200 - mse: 497.0200 - mae: 16.7765 - val_loss: 126.6211 - val_mse: 126.6211 - val_mae: 9.3736\n",
      "Epoch 46/48\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 488.9915 - mse: 488.9915 - mae: 16.6393 - val_loss: 119.3604 - val_mse: 119.3604 - val_mae: 9.0715\n",
      "Epoch 47/48\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 480.5878 - mse: 480.5878 - mae: 16.7163 - val_loss: 119.3181 - val_mse: 119.3181 - val_mae: 9.1190\n",
      "Epoch 48/48\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 479.7192 - mse: 479.7192 - mae: 16.3844 - val_loss: 134.7792 - val_mse: 134.7792 - val_mae: 9.7736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7feab5f96910>"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(10))\n",
    "\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mse','mae'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=48,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_76 (Dense)            (None, 256)               5888      \n",
      "                                                                 \n",
      " dropout_50 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 64)                16448     \n",
      "                                                                 \n",
      " dropout_51 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,986\n",
      "Trainable params: 22,986\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 134.77923583984375\n",
      "Test mse: 134.77923583984375\n",
      "Test mse: 9.773576736450195\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test mse:', score[1])\n",
    "print('Test mae:', score[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Test_list = []\n",
    "Test_list = list(Test_list)\n",
    "i = 0\n",
    "while i < len(X_test):\n",
    "    Test_list.append(model.predict(X_test)[i].mean())\n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "Test = Test_list\n",
    "Actual = Y_test\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'Test': Test, 'Actual': Actual})\n",
    "df.head()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(x=Test,y=Actual)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([114,  52,  86,  26, 114, 189, 162, 199, 192, 177, 137, 156,  71,\n",
       "       166, 176, 188, 169, 168, 120, 207, 170, 223, 177,  79, 135,  83,\n",
       "       184, 143,  71, 166, 168, 176,  68,  89, 153, 145, 175, 179, 123,\n",
       "       194, 182, 114, 269, 255, 246, 277, 253, 245, 270, 142, 210, 258,\n",
       "       267, 181, 233, 285, 278, 306, 262, 289, 275, 181, 125, 131, 277,\n",
       "       270, 187, 254, 266, 246, 227, 165, 184])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73, 22)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(73, 10)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_dataset.copy()\n",
    "test_features = test_dataset.copy()\n",
    "\n",
    "train_labels = train_features.pop('riders')\n",
    "test_labels = test_features.pop('riders')\n",
    "\n",
    "train_dataset.describe().transpose()[['mean', 'std']]\n",
    "\n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(np.array(train_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer.adapt(np.array(train_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First example: [[465 241  28   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   1   0   0]]\n",
      "\n",
      "Normalized: [[ 1.88  1.69  0.62  2.47 -0.41 -0.4  -0.43 -0.41 -0.4  -0.4  -0.31 -0.34\n",
      "  -0.29 -0.26 -0.33 -0.29 -0.3  -0.31 -0.31  3.41 -0.3  -0.29]]\n"
     ]
    }
   ],
   "source": [
    "first = np.array(train_features[:1])\n",
    "\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "  print('First example:', first)\n",
    "  print()\n",
    "  print('Normalized:', normalizer(first).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    layers.Dense(units=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 68ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[169.88095 ],\n",
       "       [130.04623 ],\n",
       "       [ 28.605843],\n",
       "       [-32.940742],\n",
       "       [ 18.0998  ],\n",
       "       [ 37.615585],\n",
       "       [161.07245 ],\n",
       "       [165.79634 ],\n",
       "       [ 17.916084],\n",
       "       [ 57.532104],\n",
       "       [ 44.89984 ],\n",
       "       [155.71997 ],\n",
       "       [ 24.6958  ],\n",
       "       [ 99.69212 ],\n",
       "       [205.25566 ],\n",
       "       [-29.470108],\n",
       "       [ 53.91673 ],\n",
       "       [ 55.33808 ],\n",
       "       [135.37787 ],\n",
       "       [233.06387 ],\n",
       "       [ 41.718796]], dtype=float32)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.predict(train_features[:21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'dense_57/kernel:0' shape=(22, 1) dtype=float32, numpy=\n",
       "array([[-0.2178182 ],\n",
       "       [ 0.27047294],\n",
       "       [-0.28908038],\n",
       "       [-0.14175645],\n",
       "       [ 0.13962418],\n",
       "       [-0.1042195 ],\n",
       "       [-0.49665806],\n",
       "       [ 0.37704277],\n",
       "       [-0.17416713],\n",
       "       [-0.2823618 ],\n",
       "       [ 0.45601285],\n",
       "       [ 0.13999486],\n",
       "       [ 0.419887  ],\n",
       "       [-0.3315292 ],\n",
       "       [ 0.0741834 ],\n",
       "       [ 0.23845708],\n",
       "       [ 0.33324766],\n",
       "       [ 0.31468832],\n",
       "       [ 0.31503272],\n",
       "       [-0.31716117],\n",
       "       [-0.11157376],\n",
       "       [-0.17622486]], dtype=float32)>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.layers[1].kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "    loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.69 s, sys: 319 ms, total: 4.01 s\n",
      "Wall time: 3.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = linear_model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    epochs=100,\n",
    "    # Suppress logging.\n",
    "    verbose=0,\n",
    "    # Calculate validation results on 20% of the training data.\n",
    "    validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = {}\n",
    "test_results['linear_model'] = linear_model.evaluate(\n",
    "    test_features, test_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_model(norm):\n",
    "  model = keras.Sequential([\n",
    "      norm,\n",
    "      layers.Dense(64, activation='relu'),\n",
    "      layers.Dense(64, activation='relu'),\n",
    "      layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3edbd7b23b0fac25ed01096b34e5f59511cb659c0b1d4bb21fa4d1eb2f403c11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
